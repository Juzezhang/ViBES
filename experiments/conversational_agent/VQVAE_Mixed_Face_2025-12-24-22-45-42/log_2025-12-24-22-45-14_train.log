2025-12-24 22:45:14,959 SEED_VALUE: 1234
DEBUG: false
FULL_CONFIG: false
TRAIN:
  SPLIT: train
  NUM_WORKERS: 16
  BATCH_SIZE: 128
  END_EPOCH: 999999
  RESUME: ''
  PRETRAINED_VAE: ''
  PRETRAINED: ''
  OPTIM:
    target: Adam
    params:
      lr: 0.0001
      betas:
      - 0.9
      - 0.99
      weight_decay: 0.0
  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1.0e-06
  STAGE: vq
  PRETRAINED_VQ: ''
  PRECISION: 32
  FORCE_BF16: false
  Loss_6D: false
  Loss_mesh: false
EVAL:
  SPLIT: test
  BATCH_SIZE: 128
  NUM_WORKERS: 8
TEST:
  CHECKPOINTS: ''
  SPLIT: test
  BATCH_SIZE: 1
  NUM_WORKERS: 8
  SAVE_PREDICTIONS: false
  COUNT_TIME: false
  REPLICATION_TIMES: 20
  REP_I: 0
  FOLDER: results
model:
  target: conver_agent.models.conversational_agent.Conversational_Agent
  params:
    condition: text
    task: t2m
    lm: ${lm.default}
    motion_vae: ${vq.default}
    stage: ${TRAIN.STAGE}
    debug: ${DEBUG}
    codebook_size: ${model.params.motion_vae.params.code_num}
    motion_codebook_size: 256
    audio_codebook_size: 500
    metrics_dict: ${METRIC.TYPE}
    modality_setup:
      params:
        audio_fps: ${DATASET.audio_fps}
        audio_down: ${DATASET.audio_down}
        motion_fps: ${DATASET.pose_fps}
        motion_down: ${DATASET.unit_length}
    modality_tokenizer:
      vae_face:
        target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
        params:
          vae_layer: 3
          code_num: 512
          codebook_size: 512
          vae_quantizer_lambda: 1
          vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
  whisper_path: ../language_of_motion/model_files/whisper-large-v2
LOSS:
  LAMBDA_REC: 1.0
  LAMBDA_JOINT: 1.0
  LAMBDA_LATENT: 1.0e-05
  LAMBDA_KL: 1.0e-05
  LAMBDA_GEN: 1.0
  LAMBDA_CROSS: 1.0
  LAMBDA_CYCLE: 1.0
  LAMBDA_PRIOR: 0.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  ABLATION:
    RECONS_LOSS: l1_smooth
  LAMBDA_FEATURE: 1.0
  LAMBDA_CLS: 1.0
METRIC:
  TASK: t2m
  FORCE_IN_METER: true
  DIST_SYNC_ON_STEP: true
  MM_NUM_SAMPLES: 100
  MM_NUM_REPEATS: 30
  MM_NUM_TIMES: 10
  DIVERSITY_TIMES: 300
  TM2T:
    t2m_path: ../language_of_motion/model_files/t2m_evaluators/t2m/
  TYPE:
  - RotationMetrics
  CO_SPEECH:
    e_path: weights/AESKConv_240_100.bin
    params:
      vae_test_len: 32
      vae_test_dim: 330
      vae_test_stride: 20
      vae_length: 240
      vae_codebook_size: 256
      vae_layer: 4
      vae_grow:
      - 1
      - 1
      - 2
      - 1
      variational: false
      vae_quantizer_lambda: 1.0
      e_name: VAESKConv
      data_path_1: ../language_of_motion/model_files/
DATASET:
  target: multimodal_tokenizers.data.MixedDataset.MixedDataModule
  CODE_PATH: VQVAE
  TASK_ROOT: ''
  TASK_PATH: ''
  NFEATS: 263
  KIT:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 24
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 12.5
    UNIT_LEN: 4
  HUMANML3D:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 20.0
    UNIT_LEN: 4
    STD_TEXT: false
    ROOT: /scr/juze/datasets/HumanML3D
    SPLIT_ROOT: /scr/juze/datasets/HumanML3D
    MEAN_STD_PATH: models/t2m/
  BEAT2:
    MAX_MOTION_LEN: 612
    MIN_MOTION_LEN: 4
    UNIT_LEN: 4
    AUDIO_UNIT_LEN: 640
    STD_TEXT: false
    ROOT: /scr/juze/datasets/BEAT2/beat_english_v2.0.0
  vary_length: false
  pose_fps: 25
  unit_length: 4
  pre_frames: 4
  audio_fps: 16000
  audio_down: 640
  motion_representation: ${Selected_type}
  FLAME_PATH: ../language_of_motion/model_files/FLAME2020/
  datasets:
  - name: BEAT2
    code_path_audio: audios_token_25hz
    stride: 20
    pose_length: 64
    pose_fps: 25
    training_speakers:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23
    - 24
    - 25
    - 26
    - 27
    - 28
    - 29
    - 30
    testing_speakers:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23
    - 24
    - 25
    - 26
    - 27
    - 28
    - 29
    - 30
    additional_data: true
    pose_rep: smplxflame_25
  - name: CANDOR
    pose_length: 64
    stride: 20
  - name: TFHP
    pose_length: 64
    stride: 20
  - name: YouTube_Talking
    pose_length: 64
    stride: 20
  - name: YouTube_Talking_Synthetic
    pose_length: 64
    stride: 20
  SMPL_PATH: ../language_of_motion/model_files/smplx_models
  TRANSFORM_PATH: ../language_of_motion/model_files/transforms/
  WORD_VERTILIZER_PATH: ../language_of_motion/model_files/t2m_evaluators/glove/
  AMASS:
    ROOT: /scr/juze/datasets/HumanML3D_25
  AMASS_talking:
    ROOT: /scr/juze/datasets/AMASS_talking
  AMASS_P2P:
    ROOT: /scr/juze/datasets/AMASS_p2p
  LIBRISPEECH:
    ROOT: /scr/juze/datasets/LibriSpeech
  CANDOR:
    ROOT: /simurgh/u/juze/datasets/CANDOR
  TFHP:
    ROOT: /simurgh/u/juze/datasets/TFHP
  YouTube_Talking:
    ROOT: /simurgh/group/juze/datasets/YouTube_Talking
  YouTube_Talking_Synthetic:
    ROOT: /simurgh/group/juze/datasets/YouTube_Talking_Synthetic
  BEAT2_AMASS:
    ROOT: /scr/juze/datasets/beat2_amass
    MEAN_STD_PATH: ../language_of_motion/model_files/t2m_evaluators/
ABLATION:
  use_length: false
  predict_ratio: 0.2
  inbetween_ratio: 0.25
  image_size: 256
  VAE_TYPE: actor
  VAE_ARCH: encoder_decoder
  PE_TYPE: actor
  DIFF_PE_TYPE: actor
  SKIP_CONNECT: false
  MLP_DIST: false
  IS_DIST: false
  PREDICT_EPSILON: true
DEMO:
  EXAMPLE: null
  TASK: t2m
LOGGER:
  VAL_EVERY_STEPS: 5
  LOGGERS:
  - tensorboard
  - wandb
  TENSORBOARD:
    target: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: ${FOLDER_EXP}
      name: tensorboard
      version: ''
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: conversational_agent
      offline: false
      id: null
      version: ''
      name: ${NAME}
      save_dir: ${FOLDER_EXP}
  TYPE:
  - wandb
NAME: VQVAE_Mixed_Face
ACCELERATOR: gpu
NUM_NODES: 1
DEVICE:
- 0
Selected_type: separate_rot
Representation_type:
  separate_rot:
    face:
      ori: beat_smplx_joints
      tar: beat_smplx_face_head
      vae_test_dim: 112
lm:
  conversational_agent_glm:
    target: conver_agent.archs.conver_agent_glm.ConverAgentGLM
    params:
      model_path: THUDM/glm-4-voice-9b
      tokenizer_path: THUDM/glm-4-voice-tokenizer
      flow_path: ./glm-4-voice-decoder
      use_quantization: false
      stage: ${TRAIN.STAGE}
      flash_attention: true
      max_length: 1024
      motion_down_sampling: ${DATASET.unit_length}
      audio_down_sampling: ${DATASET.audio_down}
      motion_framerate: ${DATASET.pose_fps}
      audio_samplerate: ${DATASET.audio_fps}
      lora: true
      lora_rank: 16
      lora_alpha: 32
      lora_dropout: 0.05
      lora_target_modules:
      - query_key_value
      - dense
      - dense_h_to_4h
      - dense_4h_to_h
      new_token_type: insert
      quota_ratio: 0.5
      noise_density: 0.15
      mean_noise_span_length: 3
      modalities:
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: ftoken
        audio:
          codebook_size: 1024
          prefix: atoken
  conversational_agent_llama3 copy:
    target: conver_agent.archs.conversational_agent.MLM
    params:
      model_type: llama3
      model_path: meta-llama/Llama-3.1-8B-Instruct
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.motion_codebook_size}
      audio_codebook_size: ${model.params.audio_codebook_size}
      flash_attention: true
      max_length: 512
      motion_down_sampling: 1
      audio_down_sampling: 320
      motion_framerate: 30.0
      audio_samplerate: 16000.0
      lora: true
      new_token_type: insert
      modalities:
        motion:
          codebook_size: 0
          prefix: motion_id
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: face_id
        hand:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: hand_id
        upper:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: upper_id
        lower:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: lower_id
        audio:
          codebook_size: 502
          prefix: audio_id
  conversational_agent_qwen:
    target: conver_agent.archs.conver_agent_qwen.ConverAgentQwen
    params:
      model_path: ./model_files/qwen2.5-0.5b-instruct
      use_quantization: false
      stage: ${TRAIN.STAGE}
      flash_attention: false
      max_length: 1024
      motion_down_sampling: ${DATASET.unit_length}
      audio_down_sampling: ${DATASET.audio_down}
      motion_framerate: ${DATASET.pose_fps}
      audio_samplerate: ${DATASET.audio_fps}
      lora: false
      lora_rank: 16
      lora_alpha: 32
      lora_dropout: 0.05
      lora_target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
      new_token_type: insert
      quota_ratio: 0.5
      noise_density: 0.15
      mean_noise_span_length: 3
      modalities:
        motion:
          codebook_size: 0
          prefix: Mo
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Fa
        hand:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Ha
        upper:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Up
        lower:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Lo
        audio:
          codebook_size: 500
          prefix: Hu
  conversational_agent:
    target: conver_agent.archs.conversational_agent.MLM
    params:
      model_type: llama
      model_path: ./models/meta-llama/Llama-3.1-8B-Instruct
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.motion_codebook_size}
      audio_codebook_size: ${model.params.audio_codebook_size}
      flash_attention: false
      ablation: ${ABLATION}
      max_length: 512
      motion_down_sampling: 1
      audio_down_sampling: 320
      motion_framerate: 30.0
      audio_samplerate: 16000.0
      modalities:
        motion:
          codebook_size: 0
          prefix: motion_id
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: face_id
        hand:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: hand_id
        upper:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: upper_id
        lower:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: lower_id
        audio:
          codebook_size: 502
          prefix: audio_id
  default:
    target: conver_agent.archs.lom.MLM
    params:
      model_type: t5
      model_path: /simurgh/u/juze/code/language_of_motion/model_files/t5_models/flan-t5-base
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
      max_length: 512
      motion_down_sampling: 1
      audio_down_sampling: 320
      audio_samplerate: 16000.0
      modalities:
        motion:
          codebook_size: 0
          prefix: motion_id
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: face_id
        hand:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: hand_id
        upper:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: upper_id
        lower:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: lower_id
        audio:
          codebook_size: 502
          prefix: audio_id
  conversational_agent_llama3:
    target: conver_agent.archs.conver_agent_llama.ConverAgentLlama
    params:
      model_type: llama3
      model_path: ./model_files/Llama-3.2-1B-Instruct
      stage: ${TRAIN.STAGE}
      flash_attention: false
      max_length: 1024
      motion_down_sampling: 1
      audio_down_sampling: 640
      motion_framerate: 30.0
      audio_samplerate: 16000.0
      lora: false
      lora_rank: 16
      lora_alpha: 32
      lora_dropout: 0.05
      lora_target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      new_token_type: insert
      modalities:
        motion:
          codebook_size: 0
          prefix: Mo
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Fa
        hand:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Ha
        upper:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Up
        lower:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: Lo
        audio:
          codebook_size: 500
          prefix: Hu
  conversational_agent_opensource:
    target: conver_agent.archs.conversational_agent.MLM
    params:
      model_type: llama3
      model_path: mistralai/Mistral-7B-Instruct-v0.2
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.motion_codebook_size}
      audio_codebook_size: ${model.params.audio_codebook_size}
      flash_attention: true
      max_length: 512
      motion_down_sampling: 1
      audio_down_sampling: 320
      motion_framerate: 30.0
      audio_samplerate: 16000.0
      lora: true
      new_token_type: insert
      modalities:
        motion:
          codebook_size: 0
          prefix: motion_id
        face:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: face_id
        hand:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: hand_id
        upper:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: upper_id
        lower:
          codebook_size: ${vq.lom_vq_motion.params.codebook_size}
          prefix: lower_id
        audio:
          codebook_size: 502
          prefix: audio_id
mics:
  config_get_face_code_512_512_ds1_wo_meshloss:
    NAME: Get_Face_Code_Artalk
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS1_Dim_512_wo_meshloss/checkpoints/epoch=79.ckpt
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: YouTube_Talking
        code_path: TOKENS_DS4_512_512_DS1_wo_meshloss
    METRIC:
      TYPE: []
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_lower_global_code_256_512_ds4_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Lower_Global_256_512_ds4_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 2
    Selected_part: lower_global
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_global:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 1
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - BodyMetrics
      - GlobalMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_lower:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 20
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_32:
    NAME: VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_32
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 3
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_32
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 32
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_lower_code_256_512_ds4_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Lower_256_512_ds4_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 2
    Selected_part: lower_54
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_AMASS_Lower_256_512_ds4_wo_mesh_lr1e-4
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - BodyMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_lower:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 20
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_upper_code_256_512_ds4_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Upper_256_512_ds4_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 3
    Selected_part: upper
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_AMASS_Upper_256_512_ds4_wo_mesh_lr1e-4_2025-08-29-00-10-14
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 1
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - BodyMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_upper:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_get_upper_code_256_512_ds4_wo_meshloss:
    NAME: Get_Upper_Code
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_part: upper
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: model_files/vq_checkpoints/upper/epoch=129.ckpt
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 1
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
        code_path: TOKENS_256_512_DS4_wo_meshloss
    METRIC:
      TYPE:
      - BodyMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_upper:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds4_dim_512:
    NAME: VQVAE_Mixed_Face_Only_512_DS4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 1
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS4
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_256:
    NAME: VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_256
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 1
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_64:
    NAME: VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_64
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 1
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_64
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 64
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_512_wo_meshloss_synthetic:
    NAME: VQVAE_Mixed_Face_Only_512_DS1_Dim_512_wo_meshloss_synthetic_combined
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 3
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED_VQ: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS1_Dim_512_wo_meshloss/checkpoints/epoch=79.ckpt
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      Loss_mesh: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
      - name: YouTube_Talking_Synthetic
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 5
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face copy:
    NAME: VQVAE_Mixed_Face_Only_Moredata
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    - 1
    - 2
    - 3
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ./experiments/conversational_agent/VQVAE_Mixed_Face_Only
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 1
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
        code_path: TOKENS_Speaker2
        code_path_audio: audios_token_glm
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_1024_ds1_dim_512:
    NAME: VQVAE_Mixed_Face_Only_Size_1024_DS1_Dim_512
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 1024
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds2_dim_512:
    NAME: VQVAE_Mixed_Face_Only_512_DS2
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 2
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS2_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_512:
    NAME: VQVAE_Mixed_Face_Only_512_DS1
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS1
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_glm_training:
    NAME: GLM4Voice_Training
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face
          vae_test_dim: 106
        hand:
          ori: beat_smplx_joints
          tar: beat_smplx_hand
          vae_test_dim: 180
        upper:
          ori: beat_smplx_joints
          tar: beat_smplx_upper
          vae_test_dim: 78
        lower:
          ori: beat_smplx_joints
          tar: beat_smplx_lower
          vae_test_dim: 54
    TRAIN:
      STAGE: lm_causal_instruct
      NUM_WORKERS: 8
      BATCH_SIZE: 16
      END_EPOCH: 999999
      PRECISION: bf16
      FORCE_BF16: false
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: /simurgh/u/juze/code/exp_motion/language_of_motion/models/pretrained_vq_emage/vq_emage_speaker_2.ckpt
      LORA:
        enabled: true
        rank: 16
        alpha: 32
        dropout: 0.05
        target_modules:
        - query_key_value
        - dense
        - dense_h_to_4h
        - dense_4h_to_h
      OPTIM:
        target: AdamW
        params:
          lr: 5.0e-05
          betas:
          - 0.9
          - 0.99
          weight_decay: 0.01
      LR_SCHEDULER:
        target: CosineAnnealingLR
        params:
          T_max: 20000
          eta_min: 1.0e-06
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
      SAVE_PREDICTIONS: true
      REPLICATION_TIMES: 1
    DATASET:
      vary_length: false
      pose_fps: 30
      unit_length: 1
      pre_frames: 4
      test_length: 120
      audio_fps: 16000
      audio_down: 320
      motion_representation: ${Selected_type}
      target: Conver_Agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        code_path_audio: audios_token
        stride: 20
        training_speakers:
        - 2
        testing_speakers:
        - 2
        additional_data: true
        pose_rep: smplxflame_30
        code_path: TOKENS_Speaker2
        instructions_file: template_instructions_beat2_minimal.json
    METRIC:
      TYPE:
      - CoSpeechMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: Conver_Agent.models.glm_trainer.GLMTrainer
      params:
        task: a2m
        lm_config:
          params:
            model_path: THUDM/glm-4-9b
            flash_attention: true
            max_length: 512
            use_quantization: true
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: Conver_Agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
          vae_hand:
            target: Conver_Agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.hand.vae_test_dim}
          vae_upper:
            target: Conver_Agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.upper.vae_test_dim}
          vae_lower:
            target: Conver_Agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.lower.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: language_motion
          name: ${NAME}
  config_mixed_stage3_exp:
    NAME: Instruct_Mixed_Exp
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    TRAIN:
      STAGE: lm_instruct
      NUM_WORKERS: 32
      BATCH_SIZE: 10
      END_EPOCH: 999999
      PRECISION: bf16
      FORCE_BF16: false
      RESUME: ''
      PRETRAINED: experiments/emgpt_emage/Pretrain_Mixed_Exp/checkpoints/epoch=19.ckpt
      PRETRAINED_VAE_FACE: ../emage/EMAGE/pretrained_vq/last_790_face_v2.bin
      PRETRAINED_VAE_HAND: ./deps/pretrained_vq_exp/hand/last_87.bin
      PRETRAINED_VAE_UPPER: ./deps/pretrained_vq_exp/upper/last_87.bin
      PRETRAINED_VAE_LOWER: ./deps/pretrained_vq_exp/lower/last_84.bin
      PRETRAINED_VAE_GLOBAL: ./deps/pretrained_vq_exp/global/pre/last_360.bin
      OPTIM:
        target: AdamW
        params:
          lr: 0.0002
          betas:
          - 0.9
          - 0.99
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: experiments/emgpt_emage/Instruct_Mixed_Exp_2024-11-07-20-37-15/checkpoints/epoch=279.ckpt
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      target: mGPT.data.MixedDataset.MixedDataModule
      datasets:
      - name: AMASS
        data_root: /nas/nas_32/AI-being/zhangjz/exp_motion/datasets/AMASS/
        type: amass
        code_path: TOKENS
        instructions_file: template_instructions_amass.json
      - name: BEAT2
        data_root: /nas/nas_32/AI-being/zhangjz/exp_motion/datasets/beat2_original/beat_v2.0.0/beat_english_v2.0.0/
        type: beat2
        code_path: TOKENS
        instructions_file: template_instructions_beat2.json
      face_vae_layer: 2
      face_vae_length: 256
      face_vae_test_dim: 106
      hand_vae_layer: 2
      hand_vae_length: 256
      hand_vae_test_dim: 180
      upper_vae_layer: 2
      upper_vae_length: 256
      upper_vae_test_dim: 78
      lower_vae_layer: 2
      lower_vae_length: 256
      lower_vae_test_dim: 54
      global_vae_layer: 4
      global_vae_length: 256
      global_vae_test_dim: 61
      vary_length: true
      CODE_PATH: TOKENS
      smpl_path: deps/smpl_models
      training_speakers:
      - 2
      additional_data: false
      cache_path: datasets/beat_cache/beat_smplx_en_emage/
      e_path: weights/AESKConv_240_100.bin
      e_name: VAESKConv
      new_cache: false
      multi_length_training:
      - 1.0
      disable_filtering: false
      clean_first_seconds: 0
      clean_final_seconds: 0
      selected_file: None
      ori_joints: beat_smplx_joints
      tar_joints: beat_smplx_full
      pose_rep: smplxflame_30
      pose_norm: false
      pose_fps: 30
      rot6d: true
      pre_frames: 4
      pose_dims: 330
      pose_length: 120
      stride: 40
      test_length: 120
      motion_f: 256
      m_pre_encoder: null
      m_encoder: null
      m_fix_pre: false
      beat_align: true
      facial_rep: smplxflame_30
      facial_dims: 100
      facial_norm: false
      facial_f: 0
      f_pre_encoder: null
      f_encoder: null
      f_fix_pre: false
      id_rep: onehot
      speaker_f: 0
      audio_rep: onset+amplitude
      audio_sr: 16000
      audio_fps: 16000
      audio_norm: false
      audio_f: 256
      word_rep: textgrid
      word_index_num: 11195
      word_dims: 300
      freeze_wordembed: false
      word_f: 256
      t_pre_encoder: fasttext
      t_encoder: null
      t_fix_pre: false
    METRIC:
      TYPE:
      - AM2AMetrics_Exp
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: mGPT.models.emgpt_emage.ExpressionalMotionGPT
      params:
        condition: audio
        task: a2m
        lm: ${lm.emgpt_flash_attention}
        motion_vae: ${vq.emage}
        audio_setup:
          params:
            audio_samplerate: 16000
            audio_down: 320
    LOGGER:
      TYPE:
      - tensorboard
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: motiongpt
  config_mixed_stage1_vq_face_size_256_ds1_dim_256:
    NAME: VQVAE_Mixed_Face_Only_256_DS1
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 3
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_256_DS1
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 2
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_get_face_code_512_512_ds1:
    NAME: Get_Face_Code_Artalk
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS1/checkpoints/epoch=19.ckpt
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: TFHP
        code_path: TOKENS_DS4_moredata
    METRIC:
      TYPE: []
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_256_ds2_dim_256:
    NAME: VQVAE_Mixed_Face_Only_256_DS2
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 1
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_256_DS2
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 2
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS2_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_get_face_code:
    NAME: Get_Face_Code_Artalk
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS1_Dim_512_wo_meshloss/checkpoints/epoch=79.ckpt
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: TFHP
        code_path: TOKENS_DS4_moredata
    METRIC:
      TYPE: []
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_256_ds4_dim_256:
    NAME: VQVAE_Mixed_Face_Only_256_DS4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_256_DS4
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_128:
    NAME: VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_128
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 1
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_Size_512_DS1_Dim_128
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 128
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vae_global_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Global_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_part: global
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      global:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vae
      NUM_WORKERS: 4
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_AMASS_Global_wo_mesh_lr1e-4
      PRETRAINED: ''
      PRECISION: '32'
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: ./models/pretrained_vq_exp_ds/upper/last_325.bin
      SPLIT: test
      TEST_LENGTH: 120
      BATCH_SIZE: 32
    DATASET:
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      vary_length: false
      pose_fps: 25
      e_path: weights/AESKConv_240_100.bin
      e_name: VAESKConv
      audio_fps: 16000
      audio_down: 640
      unit_length: 1
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - GlobalMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_global:
            target: conver_agent.archs.lom_vq.VAEConvZero
            params:
              vae_layer: 4
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: 61
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_lower_code_256_512_ds1_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Lower_256_512_ds1_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_part: lower_54
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ_LOWER: ''
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - BodyMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_lower:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_upper_code_256_512_ds1_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Upper_256_512_ds1_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_part: upper
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ_UPPER: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_AMASS_Upper_256_512_ds1_wo_mesh_lr1e-4_2025-08-28-01-32-13/checkpoints/epoch=389.ckpt
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - BodyMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_upper:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_get_face_code_256_256_ds4:
    NAME: Get_Face_Code_Artalk
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: ./model_files/face_tokenizer_moredata/epoch=179.ckpt
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: TFHP
        code_path: TOKENS_DS4_moredata
    METRIC:
      TYPE: []
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_compositional:
    NAME: VQVAE_Mixed_Compositional
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_part: compositional
    Representation_type:
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
      full_rot:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      compositional:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face
          vae_test_dim: 112
        hand:
          ori: beat_smplx_joints
          tar: beat_smplx_hand
          vae_test_dim: 180
        upper:
          ori: beat_smplx_joints
          tar: beat_smplx_upper
          vae_test_dim: 78
        lower:
          ori: beat_smplx_joints
          tar: beat_smplx_lower
          vae_test_dim: 54
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: ''
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 1
      SPLIT: test
    TEST:
      CHECKPOINTS_FACE: /simurgh/u/juze/code/conversational_agent/model_files/pretrained_cpt/lom_vq_ds_new/face/epoch=29.ckpt
      CHECKPOINTS_HAND: /simurgh/u/juze/code/conversational_agent/model_files/pretrained_cpt/lom_vq_ds_new/lom_vq.ckpt
      CHECKPOINTS_UPPER: /simurgh/u/juze/code/conversational_agent/model_files/pretrained_cpt/lom_vq_ds_new/lom_vq.ckpt
      CHECKPOINTS_LOWER: /simurgh/u/juze/code/conversational_agent/model_files/pretrained_cpt/lom_vq_ds_new/lom_vq.ckpt
      CHECKPOINTS_GLOBAL: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: YouTube_Talking_Upper
        code_path: TOKENS_AGENT_25
      datasets_test:
      - name: YouTube_Talking_Upper
        code_path: TOKENS_AGENT_25
    METRIC:
      TYPE: []
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_part}.face.vae_test_dim}
          vae_upper:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_part}.upper.vae_test_dim}
          vae_lower:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_part}.lower.vae_test_dim}
          vae_hand:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 256
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_part}.hand.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_face_size_512_ds1_dim_512_wo_meshloss:
    NAME: VQVAE_Mixed_Face_Only_512_DS1_Dim_512_wo_meshloss
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 1
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      Loss_mesh: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: BEAT2
        code_path_audio: audios_token_25hz
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        testing_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: CANDOR
        pose_length: 64
        stride: 20
      - name: TFHP
        pose_length: 64
        stride: 20
      - name: YouTube_Talking
        pose_length: 64
        stride: 20
    METRIC:
      TYPE:
      - RotationMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_get_face_code_512_512_ds1_wo_meshloss_synthetic:
    NAME: Get_Face_Code_Artalk
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    Selected_type: separate_rot
    Representation_type:
      separate_rot:
        face:
          ori: beat_smplx_joints
          tar: beat_smplx_face_head
          vae_test_dim: 112
    TRAIN:
      STAGE: vq
      NUM_WORKERS: 16
      BATCH_SIZE: 128
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRETRAINED_VQ: /simurgh/u/juze/code/conversational_agent/experiments/conversational_agent/VQVAE_Mixed_Face_Only_512_DS1_Dim_512_wo_meshloss_synthetic_combined/checkpoints/epoch=29.ckpt
      PRECISION: 32
      FORCE_BF16: false
      Loss_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 128
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      unit_length: 4
      pre_frames: 4
      audio_fps: 16000
      audio_down: 640
      motion_representation: ${Selected_type}
      target: conver_agent.data.MixedDataset.MixedDataModule
      FLAME_PATH: ./model_files/FLAME2020/
      datasets:
      - name: YouTube_Talking_Synthetic
        code_path: TOKENS_512_512_DS1_wo_meshloss_synthetic
    METRIC:
      TYPE: []
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        condition: text
        task: t2m
        modality_setup:
          params:
            audio_fps: ${DATASET.audio_fps}
            audio_down: ${DATASET.audio_down}
            motion_fps: ${DATASET.pose_fps}
            motion_down: ${DATASET.unit_length}
        modality_tokenizer:
          vae_face:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 512
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Representation_type.${Selected_type}.face.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
  config_mixed_stage1_vq_lower_global_code_256_512_ds1_wo_mesh_lr1e-4:
    NAME: VQVAE_AMASS_Lower_Global_256_512_ds1_wo_mesh_lr1e-4
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 3
    Selected_part: lower_global
    Body_parts:
      upper:
        ori: beat_smplx_joints
        tar: beat_smplx_upper
        vae_test_dim: 78
      lower:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_global:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 61
      lower_54:
        ori: beat_smplx_joints
        tar: beat_smplx_lower
        vae_test_dim: 54
      full:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 330
      full_new_loss:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 337
      full_h3d:
        ori: beat_smplx_joints
        tar: beat_smplx_full
        vae_test_dim: 263
    TRAIN:
      STAGE: vqvae
      NUM_WORKERS: 16
      BATCH_SIZE: 32
      END_EPOCH: 999999
      RESUME: ''
      PRETRAINED: ''
      PRECISION: 32
      FORCE_BF16: false
      LOSS_MESH: false
      LOSS_6D: false
      OPTIM:
        target: Adam
        params:
          lr: 0.0001
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 1
      SPLIT: test
    TEST:
      CHECKPOINTS: ''
      SPLIT: test
      BATCH_SIZE: 1
    DATASET:
      vary_length: false
      pose_fps: 25
      motion_representation: rotation
      target: conver_agent.data.MixedDataset.MixedDataModule
      datasets:
      - name: BEAT2
        stride: 20
        pose_length: 64
        pose_fps: 25
        training_speakers:
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        additional_data: true
        pose_rep: smplxflame_25
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
      datasets_test:
      - name: AMASS
        stride: 20
        pose_length: 64
        pose_fps: 25
    METRIC:
      TYPE:
      - BodyMetrics
      - GlobalMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: conver_agent.models.conversational_agent.Conversational_Agent
      params:
        task: t2m
        modality_setup:
          params:
            audio_fps: ${lm.conversational_agent.params.audio_samplerate}
            audio_down: ${lm.conversational_agent.params.audio_down_sampling}
            motion_fps: ${lm.conversational_agent.params.motion_framerate}
            motion_down: ${lm.conversational_agent.params.motion_down_sampling}
        modality_tokenizer:
          vae_lower:
            target: conver_agent.archs.lom_vq.VQVAEConvZeroDSUS1_PaperVersion
            params:
              vae_layer: 3
              code_num: 256
              codebook_size: 512
              vae_quantizer_lambda: 1
              vae_test_dim: ${Body_parts.${Selected_part}.vae_test_dim}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: conversational_agent
vq:
  default:
    target: lom.archs.mgpt_vq.VQVae
    params:
      quantizer: ema_reset
      code_num: 512
      code_dim: 512
      output_emb_width: 512
      down_t: 2
      stride_t: 2
      width: 512
      depth: 3
      dilation_growth_rate: 3
      norm: None
      activation: relu
      nfeats: ${DATASET.NFEATS}
      ablation: ${ABLATION}
  lom_vq_motion:
    target: lom.archs.lom_vq.VQVAEConvZeroDSUS
    params:
      vae_layer: 2
      code_num: 256
      vae_test_dim: 330
      codebook_size: 256
      vae_quantizer_lambda: 1
  emage_vq_setting:
    target: lom.archs.lom_vq.VQVAEConvZeroDSUS
    params:
      vae_layer: 2
      code_num: 256
      vae_test_dim: 330
      codebook_size: 256
      vae_quantizer_lambda: 1
  lom:
    target: mGPT.archs.motion_representation.VQVAEConvZero
    params:
      vae_test_len: 32
      vae_test_dim: 330
      vae_test_stride: 20
      vae_length: 240
      vae_codebook_size: 256
      vae_layer: 4
      vae_grow:
      - 1
      - 1
      - 2
      - 1
      variational: false
      vae_quantizer_lambda: 1.0
      e_name: VAESKConv
      data_path_1: ./models/
  emage:
    target: mGPT.archs.motion_representation.VQVAEConvZero
    params:
      vae_test_len: 32
      vae_test_dim: 330
      vae_test_stride: 20
      vae_length: 240
      vae_codebook_size: 256
      vae_layer: 4
      vae_grow:
      - 1
      - 1
      - 2
      - 1
      variational: false
      vae_quantizer_lambda: 1.0
      e_name: VAESKConv
      data_path_1: ./models/
CONFIG_FOLDER: configs
FOLDER: ./experiments
RENDER:
  BLENDER_PATH: /root/blender-2.93.18-linux-x64/blender
  SMPL_MODEL_PATH: ../language_of_motion/model_files/smpl/smpl_models/smpl
  SMPLX2020_MODEL_PATH: ../language_of_motion/model_files/smpl_models
  MODEL_PATH: ../language_of_motion/model_files/smpl_models
  FACES_PATH: ../language_of_motion/model_files/smplh/smplh.faces
FOLDER_EXP: experiments/conversational_agent/VQVAE_Mixed_Face
TIME: 2025-12-24-22-45-14

2025-12-24 22:45:14,968 Callbacks initialized
